{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f4ab4c",
   "metadata": {
    "id": "57f4ab4c"
   },
   "source": [
    "<h1 align=\"center\">MSIN0114: Business Analytics Consulting Project</h1>\n",
    "<h2 align=\"center\">S2R Analytics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892fa8b",
   "metadata": {
    "id": "e892fa8b"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "**Data enginering**\n",
    "\n",
    "* [Part 0](#part0): Data extraction\n",
    "\n",
    "* [Part 1](#part1): Data transformation\n",
    "    * [1.1](#1_1): Projects\n",
    "    * [1.2](#1_2): Clients\n",
    "    * [1.3](#1_3): Stages\n",
    "    * [1.4](#1_4): Transactions\n",
    "    * [1.5](#1_5): Data health\n",
    "    * [1.6](#1_6): Staff\n",
    "    * [1.7](#1_7): Clean-up    \n",
    " <br />\n",
    "* [Part 2](#part2): Data loading\n",
    "    * [2.1](#2_1): Database design\n",
    "    * [2.2](#2_2): Data storage\n",
    "\n",
    "**Predictive analytics**\n",
    "\n",
    "* [Part 3](#part3): Data splitting and scaling\n",
    "* [Part 4](#part4): Model training\n",
    "* [Part 5](#part5): Performance evaluation\n",
    "* [Part 6](#part6): Feature importance and statistical tests\n",
    "* [Part 7](#part7): Converting the output\n",
    "* [Part 8](#part8): Pipeline creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p89ifKLxEcrr",
   "metadata": {
    "id": "p89ifKLxEcrr"
   },
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F8qq-kkSaAp1",
   "metadata": {
    "id": "F8qq-kkSaAp1"
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "BpTEkpFbb0yS",
   "metadata": {
    "id": "BpTEkpFbb0yS"
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from pandas.api.types import CategoricalDtype\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np; np.random.seed(2022)\n",
    "import random\n",
    "import sqlite3\n",
    "import pyodbc\n",
    "\n",
    "#Image creation and display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import pyplot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#from image import image, display\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#Other\n",
    "import itertools as it\n",
    "import io\n",
    "import os\n",
    "os.sys.path\n",
    "import sys\n",
    "import glob\n",
    "import concurrent.futures\n",
    "from __future__ import print_function\n",
    "import binascii\n",
    "import struct\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import scipy.cluster\n",
    "import datetime, time\n",
    "import functools, operator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8712833",
   "metadata": {},
   "source": [
    "## Part 0: <a class=\"anchor\" id=\"part0\"></a> Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65fe96",
   "metadata": {},
   "source": [
    "Main data is from API scripts from Jonny.\n",
    "X columns from PowerBI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb1179",
   "metadata": {
    "id": "7cdb1179"
   },
   "source": [
    "## Part 1: <a class=\"anchor\" id=\"part1\"></a> Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c2673",
   "metadata": {},
   "source": [
    "### 1.1 <a class=\"anchor\" id=\"1_1\"></a> Projects (wga.projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407addc",
   "metadata": {},
   "source": [
    "Step 1: Create a list of projects to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9cc6c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We should have 9755 projects in total.\n"
     ]
    }
   ],
   "source": [
    "# Read all projects from Synergy API\n",
    "all_projects = pd.read_csv('csv-files/wga_synergy_incremental_projects.csv')\n",
    "all_projects = all_projects[['Project ID', 'Project Number', 'Project Name', 'Is Office Project', 'Is Billable', 'Project Status']]\n",
    "\n",
    "# Projects to keep: external (i.e. client only)\n",
    "external_projects = all_projects[(all_projects['Is Office Project'] != 'Yes')]\n",
    "external_projects = external_projects[(external_projects['Is Billable'] != 'No')]\n",
    "external_ids = external_projects['Project ID'].tolist()\n",
    "\n",
    "# Projects to keep: status-based\n",
    "successful_projects = external_projects[external_projects['Project Status'].isin(['Complete', 'Active', 'Pending Invoice']) == True]\n",
    "valid_ids = successful_projects['Project ID'].tolist()\n",
    "\n",
    "# See how many unique projects we shold have\n",
    "print('We should have ' + str(len(valid_ids)) + ' projects in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef2789f",
   "metadata": {},
   "source": [
    "Step 2: Cleaning data from Synergy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a43568b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load only valid projects\n",
    "api_projects = pd.read_csv('csv-files/wga_synergy_incremental_projects.csv')\n",
    "api_projects = (api_projects[api_projects['Project ID'].isin(valid_ids)])\n",
    "\n",
    "\n",
    "# Drop unnecesary columns\n",
    "api_projects.drop(columns = ['Unnamed: 0', 'Primary Contact Name', 'Status Name', 'Organisation ID',\n",
    "                             'customFields', 'Address Line 1', 'Address Line 2', 'Project Type ID',\n",
    "                             'Primary Contact', 'Primary Contact ID', 'Project Scope', 'Address Postal Code',\n",
    "                             'Address State', 'Address Town', 'Address Google', 'Client Reference Number',\n",
    "                             'Address State Postal Code Country', 'Address Single Line', 'Project Type Code',\n",
    "                             'External Name', 'Address Longitude', 'Address Latitude',\n",
    "                             'Project Forecast Value', 'Created Date', 'Updated Date', 'Manager ID'], inplace = True)\n",
    "\n",
    "\n",
    "# Convert columns for unified style\n",
    "api_projects.rename(columns = {'Invoices':'Number of Invoices', 'Project Net Residual (Neg as Zero)':'Project Net Residual',\n",
    "                              'Start Date (Project)': 'Project Start Date', 'End Date (Project)': 'Project End Date',\n",
    "                              'Address Country':'Country', 'Project Type': 'Sector'}, inplace = True)\n",
    "api_projects['Country'].replace(['AUSTRALIA', 'AUS', 'Autralia', 'NZ', 'new zealand', 'PNG', 'samoa', 'SAMOA', 'TONGA', 'SA', 'CHINA'],\n",
    "                                ['Australia', 'Australia', 'Australia', 'New Zealand', 'New Zealand', 'Papua New Guinea', 'Samoa', 'Samoa', 'Tonga', 'Saudi Arabia', 'China'],inplace=True)\n",
    "api_projects['Project Start Date'] = pd.to_datetime(api_projects['Project Start Date'])\n",
    "api_projects['Project End Date'] = pd.to_datetime(api_projects['Project End Date'])\n",
    "\n",
    "\n",
    "# Combine minority sectors with larger ones\n",
    "api_projects['Sector'].mask(api_projects['Sector'] == 'Commercial', 'Commercial & Retail Buildings', inplace=True)\n",
    "api_projects['Sector'].mask(api_projects['Sector'] == 'Residential', 'Civic & Education Buildings', inplace=True)\n",
    "\n",
    "\n",
    "# Adding 'Due Date' and'Project Director' columns\n",
    "custom_fields = pd.read_csv('csv-files/wga_synergy_incremental_projects_custom_fields.csv')\n",
    "custom_fields = custom_fields[['PROPOSAL - Due Date', 'PROSPECT - Project Director', 'Project ID']].copy()\n",
    "custom_fields.rename(columns = {'PROSPECT - Project Director':'Project Director', 'PROPOSAL - Due Date': 'Due Date'}, inplace = True)\n",
    "custom_fields['Due Date'] = pd.to_datetime(custom_fields['Due Date'])\n",
    "api_projects = pd.merge(api_projects, custom_fields,  how='left', left_on='Project ID', right_on='Project ID')\n",
    "\n",
    "\n",
    "# Rearrange column names for easier interpretation\n",
    "api_projects = api_projects[['Project ID', 'Country',\n",
    "                             'Project Status', 'Sector',\n",
    "                             'Project Director', 'Project Manager', 'Office',\n",
    "                             'Project Start Date', 'Project End Date', 'Due Date',\n",
    "                             'Default Rate Group','Number of Invoices', 'Project Net Residual']]\n",
    "\n",
    "api_projects.head(1)\n",
    "len(api_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bdc65",
   "metadata": {},
   "source": [
    "Step 3: Cleaning transformed PowerBI data from S2R Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0085b85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3398: DtypeWarning: Columns (0,1,17,65,66) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9754"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the pre-transformed data from PowerBI\n",
    "pbi_projects = pd.read_csv('csv-files/wga_power_bi_projects.csv', encoding = 'ISO-8859-1')\n",
    "pbi_projects = pbi_projects[['Project ID', 'Project Size Sort Order',\n",
    "                             'Project Duration (Weeks)', 'Is Multi Discipline Project','Is First Client Project']]\n",
    "\n",
    "# Load only valid projects\n",
    "pbi_projects = (pbi_projects[pbi_projects['Project ID'].isin(valid_ids)])\n",
    "\n",
    "# Convert columns for unified style\n",
    "pbi_projects.rename(columns = {'Project Duration (Weeks)':'Project Duration Weeks'}, inplace = True)\n",
    "pbi_projects['Is Multi Discipline Project'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "pbi_projects['Is First Client Project'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "\n",
    "pbi_projects.head(1)\n",
    "len(pbi_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0dd8b",
   "metadata": {},
   "source": [
    "Step 4: Merge the two 'Projects' tables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75aeb104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the projects table from API and preprocesed Power BI table\n",
    "projects = pd.merge(api_projects, pbi_projects,  how='left', left_on='Project ID', right_on='Project ID')\n",
    "projects.columns = projects.columns.str.replace(' ', '_')\n",
    "projects['Project_ID'] = projects['Project_ID'].astype('float64')\n",
    "projects.head(1)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e67c2",
   "metadata": {},
   "source": [
    "**4 features to engineer:**\n",
    "* Delivered_on_Time\n",
    "* Fully_In_Lockdown\n",
    "* Partially_In_Lockdown\n",
    "* Suffered_Data_Loss (projects that started after July 2018 did not suffer from data loss, projects that ended before July 2018 did not suffer from data loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0766208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delivered_on_Time\n",
    "\n",
    "def nat_check(date):\n",
    "    if type(date) == pd._libs.tslibs.nattype.NaTType:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "Delivered_on_Time = {}\n",
    "\n",
    "for due_date in projects['Due_Date']:\n",
    "    for completed in projects['Project_End_Date']:\n",
    "        if nat_check(due_date) == True:\n",
    "            continue\n",
    "        else:\n",
    "            if due_date <= completed:\n",
    "                Delivered_on_Time[due_date] = True\n",
    "            else:\n",
    "                Delivered_on_Time[due_date] = False\n",
    "\n",
    "df_1 = pd.DataFrame([{'Due_Date': due_date, 'Delivered_on_Time': is_on_time} for (due_date, is_on_time) in Delivered_on_Time.items()])\n",
    "\n",
    "projects = projects.merge(df_1,how='left', left_on='Due_Date', right_on='Due_Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca79c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully_In_Lockdown, Partially_In_Lockdown\n",
    "\n",
    "Lockdown_Period = (pd.date_range(start='2020-03-16', end = '2020-11-21', freq='D')).to_series()\n",
    "\n",
    "projects['Start_in_Lockdown'] = projects['Project_Start_Date'].isin([Lockdown_Period])\n",
    "projects['End_in_Lockdown'] = projects['Project_End_Date'].isin([Lockdown_Period])\n",
    "dates_prep = pd.concat([projects['Start_in_Lockdown'], projects['End_in_Lockdown']], axis = 1) #axis=1 specifies horizontal stacking\n",
    "\n",
    "projects['Fully_In_Lockdown'] = pd.DataFrame(dates_prep.all(axis=1))\n",
    "projects['Partially_In_Lockdown'] = pd.DataFrame(dates_prep.any(axis=1))\n",
    "\n",
    "projects.drop(columns = ['Start_in_Lockdown', 'End_in_Lockdown'], inplace = True)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc997053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffered_Data_Loss\n",
    "\n",
    "def data_loss_check(start_date, end_date):\n",
    "    if start_date < pd.Timestamp('2018-07-15') and end_date < pd.Timestamp('2018-07-15'): #project started and ended before the acqusition\n",
    "        return False\n",
    "    elif start_date > pd.Timestamp('2018-07-15') and end_date > pd.Timestamp('2018-07-15'): #project started and ended after the acqusition\n",
    "        return False\n",
    "    elif start_date < pd.Timestamp('2018-07-15') and end_date > pd.Timestamp('2018-07-15'): #project started before the acqusition but ended after it\n",
    "        return True\n",
    "\n",
    "Data_Loss_Check = {}\n",
    "\n",
    "for start_date in projects['Project_Start_Date']:\n",
    "    for end_date in projects['Project_End_Date']:\n",
    "        if (nat_check(start_date) or nat_check(end_date)) == True:\n",
    "            continue\n",
    "        else:\n",
    "            if data_loss_check(start_date, end_date) == True:\n",
    "                Data_Loss_Check[start_date.strftime(format = '%Y-%m-%d %H:%M:%S'), end_date.strftime(format = '%Y-%m-%d %H:%M:%S')] = True\n",
    "            else:\n",
    "                Data_Loss_Check[start_date.strftime(format = '%Y-%m-%d %H:%M:%S'), end_date.strftime(format = '%Y-%m-%d %H:%M:%S')] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd1e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL: https://www.geeksforgeeks.org/python-program-to-convert-a-tuple-to-a-string/#:~:text=There%20are%20various%20approaches%20to%20convert%20a%20tuple,of%20the%20tuple%20and%20convert%20it%20into%20string.\n",
    "\n",
    "def convertTuple(tup):\n",
    "    string = ', '.join(tup)\n",
    "    return string\n",
    "\n",
    "Execution_Timeframe = Data_Loss_Check.copy()\n",
    "\n",
    "for key in Execution_Timeframe.keys():\n",
    "    Execution_Timeframe[key] = convertTuple(key)\n",
    "    \n",
    "# Changing keys of our final dictionary\n",
    "Suffered_Data_Loss = dict(zip((Execution_Timeframe.values()), (Data_Loss_Check.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87dfa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a column in 'Projects' table to create merge on\n",
    "projects['Execution_Timeframe'] = projects['Project_Start_Date'].map(str) + ', ' + projects['Project_End_Date'].map(str)\n",
    "projects['Execution_Timeframe'][0]\n",
    "type(projects['Execution_Timeframe'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ec46fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether the two future columns will merge\n",
    "list(Suffered_Data_Loss)[0] == projects['Execution_Timeframe'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c476b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Project_ID    Country Project_Status          Sector Project_Director  \\\n",
      "0    367704.0  Australia       Complete  Ports & Marine     Mark Gilbert   \n",
      "\n",
      "  Project_Manager   Office Project_Start_Date Project_End_Date Due_Date  \\\n",
      "0     David McKay  Whyalla         2015-01-07       2015-01-08      NaT   \n",
      "\n",
      "  Default_Rate_Group  Number_of_Invoices  Project_Net_Residual  \\\n",
      "0           Standard                   6                   0.0   \n",
      "\n",
      "   Project_Size_Sort_Order  Project_Duration_Weeks  \\\n",
      "0                      4.0                     4.0   \n",
      "\n",
      "  Is_Multi_Discipline_Project Is_First_Client_Project Delivered_on_Time  \\\n",
      "0                       False                    True               NaN   \n",
      "\n",
      "   Fully_In_Lockdown  Partially_In_Lockdown  \\\n",
      "0              False                  False   \n",
      "\n",
      "                        Execution_Timeframe Suffered_Data_Loss  \n",
      "0  2015-01-07 00:00:00, 2015-01-08 00:00:00              False  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.DataFrame.from_dict(Suffered_Data_Loss, orient ='index')\n",
    "df_2 = df_2.reset_index()\n",
    "df_2.rename(columns = {'index':'Execution_Timeframe', 0:'Suffered_Data_Loss'}, inplace = True)\n",
    "\n",
    "projects = projects.merge(df_2, how='left', left_on='Execution_Timeframe', right_on='Execution_Timeframe')\n",
    "projects.head(1)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7156908",
   "metadata": {},
   "source": [
    "### 1.2 <a class=\"anchor\" id=\"1_2\"></a> Clients (wga.clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2b2b3",
   "metadata": {},
   "source": [
    "Step 1: Cleaning all given data, from Synergy API and Power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cee61ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Client_ID</th>\n",
       "      <th>Created_Date</th>\n",
       "      <th>Client_Projects_Total_No</th>\n",
       "      <th>1st_Project_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10317738</td>\n",
       "      <td>2022-05-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Client_ID Created_Date  Client_Projects_Total_No  1st_Project_ID\n",
       "0   10317738   2022-05-06                       NaN             NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Cleaning data from Synergy API.\n",
    "api_clients = pd.read_csv('csv-files/wga_synergy_overnight_1_clients.csv')\n",
    "api_clients.drop(columns = {'Client Name', 'Unnamed: 0', 'Contact Type', 'Organisation ID'}, inplace = True)\n",
    "api_clients['Created Date'] = pd.to_datetime(api_clients['Created Date'])\n",
    "\n",
    "# Step 2: Cleaning transformed PowerBI data from S2R Analytics.\n",
    "pbi_clients = pd.read_csv('csv-files/wga_power_bi_clients.csv', encoding = 'ISO-8859-1')\n",
    "pbi_clients = pbi_clients[['Client ID', 'Client Projects - Total No', 'Client Projects - First Project ID']]\n",
    "pbi_clients.rename(columns = {'Client Projects - Total No': 'Client Projects Total No',\n",
    "                              'Client Projects - First Project ID':'1st Project ID'}, inplace = True)\n",
    "\n",
    "# Step 3: Merge the two 'Clients' tables together.\n",
    "clients = pd.merge(api_clients, pbi_clients,  how='left', left_on='Client ID', right_on='Client ID')\n",
    "clients.columns = clients.columns.str.replace(' ', '_')\n",
    "clients.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58117366",
   "metadata": {},
   "source": [
    "**3 features to engineer:**\n",
    "* Client_Duration_Months\n",
    "* Client_Is_Repeated\n",
    "* Client_Is_Recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a548b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Client_Is_Repeated\n",
    "clients['Client_Is_Repeated'] = clients['1st_Project_ID'].notnull()\n",
    "\n",
    "# Client_Duration_Months\n",
    "clients['Client_Duration_Months'] = datetime.now() - clients['Created_Date']\n",
    "clients['Client_Duration_Months'] = (clients['Client_Duration_Months'].astype('timedelta64[M]'))\n",
    "clients['Client_Duration_Months'].isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea9b7097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\venv\\lib\\site-packages\\pandas\\core\\frame.py:9190: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Client_Is_Recent_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  return merge(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Client_ID</th>\n",
       "      <th>Client_Projects_Total_No</th>\n",
       "      <th>1st_Project_ID</th>\n",
       "      <th>Client_Is_Repeated</th>\n",
       "      <th>Client_Duration_Months</th>\n",
       "      <th>Client_Is_Recent_x</th>\n",
       "      <th>Client_Is_Recent_y</th>\n",
       "      <th>Client_Is_Recent_x</th>\n",
       "      <th>Client_Is_Recent_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10317738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Client_ID  Client_Projects_Total_No  1st_Project_ID  Client_Is_Repeated  \\\n",
       "0   10317738                       NaN             NaN               False   \n",
       "\n",
       "   Client_Duration_Months  Client_Is_Recent_x  Client_Is_Recent_y  \\\n",
       "0                     0.0                True                True   \n",
       "\n",
       "   Client_Is_Recent_x  Client_Is_Recent_y  \n",
       "0                True                True  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Client_Is_Recent\n",
    "\n",
    "Client_Is_Recent = {}\n",
    "\n",
    "for months in clients['Client_Duration_Months']:\n",
    "    if months < 6:\n",
    "        Client_Is_Recent[months] = True\n",
    "    else:\n",
    "        Client_Is_Recent[months] = False\n",
    "         \n",
    "df_3 = pd.DataFrame(\n",
    "    [{'Client_Duration_Months': months, 'Client_Is_Recent': recent_status} for (months, recent_status) in Client_Is_Recent.items()])\n",
    "\n",
    "clients = clients.merge(df_3,how='left', left_on='Client_Duration_Months', right_on='Client_Duration_Months')\n",
    "#clients['1st_Project_ID'] = clients['1st_Project_ID'].astype(int)\n",
    "clients.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9cee98",
   "metadata": {},
   "source": [
    "### 1.3 <a class=\"anchor\" id=\"1_3\"></a> Stages (wga.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e81ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3398: DtypeWarning: Columns (4,12,20,21,23,24,25,28,35,36,39,40,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,77,86,94,95) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60135"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read only valid projects' stages\n",
    "stages = pd.read_csv('csv-files/wga_power_bi_stages.csv', encoding = 'ISO-8859-1')\n",
    "stages = (stages[stages['Project ID'].isin(valid_ids)])\n",
    "stages = stages[(stages['Stage Type'] != 'Proposal')] # We only want professional fees\n",
    "stages = stages[['Project ID', 'Stage ID',\n",
    "                 'Is Disbursement Stage', 'Stage Fee Type',\n",
    "                 'Stage Manager', 'Stage Discipline','Stage Start Date','Stage End Date']]\n",
    "\n",
    "stages['Stage Start Date'] = pd.to_datetime(stages['Stage Start Date'])\n",
    "stages['Stage End Date'] = pd.to_datetime(stages['Stage End Date'])\n",
    "stages['Is Disbursement Stage'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "stages.columns = stages.columns.str.replace(' ', '_')\n",
    "len(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fac5c9",
   "metadata": {},
   "source": [
    "**2 features to engineer:**\n",
    "* Stage_Duration_Weeks\n",
    "* Perc_of_Stages_with_Fixed_Fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "256b5269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21413"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage_Duration_Weeks\n",
    "df_4 = pd.DataFrame(stages['Stage_Start_Date'].notnull() & stages['Stage_End_Date'].notnull())\n",
    "df_4.rename(columns = {0:'checker'}, inplace = True)\n",
    "df_4 = df_4.loc[df_4['checker'] == True]\n",
    "\n",
    "stages = pd.merge(stages, df_4, left_index=True, right_index=True)\n",
    "stages['Stage_Duration_Weeks'] = ((stages['Stage_End_Date'] - stages['Stage_Start_Date']).astype('timedelta64[W]'))\n",
    "stages.drop(columns = 'checker', inplace = True)\n",
    "len(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d85cd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarfaPopova\\AppData\\Local\\Temp\\ipykernel_2968\\1473671682.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_7['Perc_of_Stages_with_Fixed_Fee'] = ((df_7['Num_Stages_Per_Fee_Type'] / df_7['Total_Num_Stages']) * 100).round(decimals = 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21413"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perc_of_Stages_with_Fixed_Fee\n",
    "df_5 = pd.DataFrame(stages.groupby(['Project_ID', 'Stage_Fee_Type'])['Stage_ID'].count()).reset_index()\n",
    "df_6 = pd.DataFrame(df_5.groupby('Project_ID')['Stage_ID'].agg('sum')).reset_index().astype(int)\n",
    "df_6.rename(columns = {'Stage_ID':'Total_Num_Stages'}, inplace = True)\n",
    "df_6 = df_5.merge(df_6,how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "df_6.rename(columns = {'Stage_ID':'Num_Stages_Per_Fee_Type'}, inplace = True)\n",
    "df_7 = df_6[(df_6['Stage_Fee_Type'] == 'Fixed fee')]\n",
    "df_7['Perc_of_Stages_with_Fixed_Fee'] = ((df_7['Num_Stages_Per_Fee_Type'] / df_7['Total_Num_Stages']) * 100).round(decimals = 2)\n",
    "stages = pd.merge(stages, df_7,  how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "stages.drop(columns = 'Stage_Fee_Type_x', inplace = True)\n",
    "stages.rename(columns = {'Stage_Fee_Type_y': 'Stage_Fee_Type'}, inplace = True)\n",
    "stages['Project_ID'] = stages['Project_ID'].astype('float64')\n",
    "stages.head(1)\n",
    "len(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13f5df32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_fee = stages[['Project_ID', 'Perc_of_Stages_with_Fixed_Fee']]\n",
    "fixed_fee = fixed_fee.drop_duplicates()\n",
    "projects = pd.merge(projects, fixed_fee,  how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "projects.head(1)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9d4d6",
   "metadata": {},
   "source": [
    "### 1.4 <a class=\"anchor\" id=\"1_4\"></a> Transactions (wga.transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d77584d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3398: DtypeWarning: Columns (3,37,46,59,60,61) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transaction_ID  Project_ID  Stage_ID Transaction_Type Rate_Type    Status  \\\n",
      "9        45264071      375028   1427835             Time     Staff  Invoiced   \n",
      "\n",
      "   Units  Value  Value_Total  Invoice_Value_Total  Actual_Cost_Total  \\\n",
      "9    1.5  300.0        450.0               491.33            360.735   \n",
      "\n",
      "   Target_Charge_Total  Standard_Cost_Total       Date  \n",
      "9              360.735                412.5 2021-08-26  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1106654"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read only valid projects' transactions from Synergy API.\n",
    "transactions = pd.read_csv('csv-files/wga_sql_transactions.csv')\n",
    "transactions = (transactions[transactions['projectId'].isin(valid_ids)])\n",
    "\n",
    "transactions = transactions[['id', 'projectId', 'stageId', 'transactionTypeId',\n",
    "                             'rateType', 'status','units',\n",
    "                             'value', 'valueTotal',\n",
    "                             'invoiceValueTotal','actualCostTotal',\n",
    "                             'targetChargeTotal', 'standardCostTotal', 'date']]\n",
    "\n",
    "transactions.rename(columns = {'id':'Transaction ID', 'projectId':'Project ID',\n",
    "                               'transactionTypeId': 'Transaction Type',\n",
    "                               'rateType': 'Rate Type', 'status': 'Status',\n",
    "                               'stageId': 'Stage ID', 'date':'Date',\n",
    "                               'invoiceValueTotal': 'Invoice Value Total',\n",
    "                               'actualCostTotal':'Actual Cost Total',\n",
    "                               'targetChargeTotal':'Target Charge Total',\n",
    "                               'standardCostTotal':'Standard Cost Total',\n",
    "                               'valueTotal':'Value Total',\n",
    "                               'units': 'Units', 'value': 'Value'}, inplace = True)\n",
    "\n",
    "transactions['Transaction Type'].replace([100, 200, 300, 400, 500, 700, 750, 800],\n",
    "                                         ['Time', 'Cash', 'Travel', 'Office', 'Bill', 'Balance', 'Unearned', 'Invoice Custom'], inplace=True)\n",
    "transactions['Date'] = pd.to_datetime(transactions['Date'])\n",
    "transactions.columns = transactions.columns.str.replace(' ', '_')\n",
    "projects['Project_ID'] = projects['Project_ID'].astype('float64')\n",
    "\n",
    "transactions.head(1)\n",
    "len(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677303a2",
   "metadata": {},
   "source": [
    "**3 features to engineer:**\n",
    "* Perc_of_Subcontractors (move to 'Projects' table)\n",
    "* Is_Front_Loaded (move to 'Projects' table)\n",
    "* Profitability (move to 'Projects' table) - not in the ETL pipeline, in ML pipeline instead\n",
    "\n",
    "**Table alterations:**\n",
    "* FK Time_Profile (links 'wga.projects' table on 'Project_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ec805",
   "metadata": {},
   "source": [
    "### Perc_of_Subcontractors\n",
    "* total units of subcontractors\n",
    "* sum of units where transaction type is 'bill' or 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f362c74",
   "metadata": {},
   "source": [
    "* 'Time' = Company's employees\n",
    "* 'Bill' = Hired subcontrators\n",
    "* Time + Bill = total human capital on project in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8443b48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perc_of_Subcontractors\n",
    "subs = transactions[['Project_ID', 'Units', 'Rate_Type']]\n",
    "subs = subs[(subs['Rate_Type'] == 'Subcontractor')]\n",
    "subs.drop(columns = ['Rate_Type'], inplace = True)\n",
    "subs = pd.DataFrame(subs.groupby(['Project_ID'])['Units'].count()).reset_index()\n",
    "subs.rename(columns = {'Units': 'Sub_Hours_Per_Project'}, inplace = True)\n",
    "\n",
    "total_hours = transactions[['Project_ID', 'Units', 'Transaction_Type']]\n",
    "total_hours = total_hours[(total_hours['Transaction_Type'] == 'Time') | (total_hours['Transaction_Type'] == 'Bill')]\n",
    "total_hours = pd.DataFrame(total_hours.groupby(['Project_ID'])['Units'].count()).reset_index()\n",
    "total_hours.rename(columns = {'Units': 'Total_Hours_Per_Project'}, inplace = True)\n",
    "\n",
    "df_8 = pd.merge(projects, subs, how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "df_9 = pd.merge(df_8, total_hours, how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "df_9['Sub_Hours_Per_Project'].fillna(0, inplace=True)\n",
    "df_9['Total_Hours_Per_Project'].fillna(0, inplace=True)\n",
    "df_9['Perc_of_Subcontractors'] = ((df_9['Sub_Hours_Per_Project'] / df_9['Total_Hours_Per_Project']) * 100).round(decimals = 2)\n",
    "df_9 = df_9[['Project_ID', 'Perc_of_Subcontractors']]\n",
    "\n",
    "# Add the new feature to the 'Projects' table\n",
    "projects = pd.merge(projects, df_9,  how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "projects.head(1)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef0e9bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is_Front_Loaded\n",
    "project_dates = projects[['Project_ID', 'Project_Start_Date', 'Project_End_Date']]\n",
    "df_10 = transactions[['Project_ID', 'Units', 'Date']]\n",
    "df_10 = pd.merge(df_10, project_dates, how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "\n",
    "first_half = df_10[(df_10['Date']  < df_10['Project_Start_Date'] + (df_10['Project_End_Date'] - df_10['Project_Start_Date'])/2)] # finding mid-point between 2 dates\n",
    "first_half = pd.DataFrame(first_half.groupby(['Project_ID'])['Units'].sum()).reset_index()\n",
    "first_half.rename(columns = {'Units': '1st_Half_Units'}, inplace = True)\n",
    "\n",
    "total_units = pd.DataFrame(df_10.groupby(['Project_ID'])['Units'].sum()).reset_index()\n",
    "total_units.rename(columns = {'Units': 'Total_Effort_Units'}, inplace = True)\n",
    "\n",
    "df_11 = pd.merge(total_units, first_half, how ='left', left_on='Project_ID', right_on='Project_ID')\n",
    "df_11['Perc_Being_Front'] = df_11['1st_Half_Units']/df_11['Total_Effort_Units']\n",
    "df_11['Is_Front_Loaded'] = (df_11['Perc_Being_Front']>=0.7)\n",
    "df_11 = df_11[['Project_ID', 'Is_Front_Loaded']]\n",
    "\n",
    "# Add the new feature to the 'Projects' table\n",
    "projects = pd.merge(projects, df_11,  how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "projects.head(1)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16897d55",
   "metadata": {},
   "source": [
    "### 1.5 <a class=\"anchor\" id=\"1_5\"></a> Data health (wga.health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e404917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3398: DtypeWarning: Columns (4,12,20,21,23,24,25,28,35,36,39,40,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,77,86,94,95) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60245"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load only valid projects\n",
    "health = pd.read_csv('csv-files/wga_power_bi_stages.csv', encoding = 'ISO-8859-1')\n",
    "health = (health[health['Project ID'].isin(valid_ids)])\n",
    "\n",
    "\n",
    "# Only leave columns that are relevant\n",
    "health = health[['Project ID', 'Stage ID',\n",
    "                 'Data Quality - Has Issues',\n",
    "                 'Data Quality - Has Inactive Staff Resourced', \n",
    "                 'Data Quality - Rate Group', 'Health - % Duration Complete',\n",
    "                 'Health - % Fee Used', 'Health - Stages With Alerts #']]\n",
    "\n",
    "# Convert columns for unified style\n",
    "health.rename(columns = {'Data Quality - Has Issues': 'DQ_Has_Issues',\n",
    "                         'Data Quality - Has Inactive Staff Resourced':'DQ_Has_Inactive_Staff_Resourced',\n",
    "                         'Data Quality - Rate Group':'DQ_Rate_Group',\n",
    "                         'Health - % Duration Complete':'Health_Perc_Duration_Complete',\n",
    "                         'Health - % Fee Used':'Health_Perc_Fee_Used',\n",
    "                         'Health - Stages With Alerts #':'Alerts_Total_Per_Stage'}, inplace = True)\n",
    "\n",
    "health['DQ_Has_Issues'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "health['DQ_Has_Inactive_Staff_Resourced'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "health.columns = health.columns.str.replace(' ', '_')\n",
    "\n",
    "health.head(1)\n",
    "len(health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40ab33da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project_ID</th>\n",
       "      <th>Stage_ID</th>\n",
       "      <th>Alerts_Total_Per_Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368035</td>\n",
       "      <td>1390483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26089</th>\n",
       "      <td>368035</td>\n",
       "      <td>1390482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76438</th>\n",
       "      <td>368035</td>\n",
       "      <td>1390484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76439</th>\n",
       "      <td>368035</td>\n",
       "      <td>1390485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76440</th>\n",
       "      <td>368035</td>\n",
       "      <td>1390486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Project_ID  Stage_ID  Alerts_Total_Per_Stage\n",
       "0          368035   1390483                       0\n",
       "26089      368035   1390482                       1\n",
       "76438      368035   1390484                       1\n",
       "76439      368035   1390485                       0\n",
       "76440      368035   1390486                       0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker = health[health['Project_ID'].isin([368035]) == True]\n",
    "checker = checker[['Project_ID', 'Stage_ID', 'Alerts_Total_Per_Stage']]\n",
    "checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d0550",
   "metadata": {},
   "source": [
    "**1 feature to engineer:**\n",
    "* Total_Data_Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "424b35a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alerts_Total_Per_Project\n",
    "issues = health.groupby(['Project_ID'], sort=False).sum('Alerts_Total_Per_Stage').reset_index()\n",
    "issues = issues[['Project_ID', 'Alerts_Total_Per_Stage']]\n",
    "issues.rename(columns = {'Alerts_Total_Per_Stage':'Total_Data_Issues'}, inplace = True)\n",
    "projects = projects.merge(issues, how ='left', left_on='Project_ID', right_on='Project_ID')\n",
    "projects.head(1)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed40f7",
   "metadata": {},
   "source": [
    "### 1.6 <a class=\"anchor\" id=\"1_6\"></a> Human resources (wga.staff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d70a064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staff = pd.read_csv('csv-files/wga_synergy_overnight_1_staff.csv')\n",
    "staff = staff[['Staff Name', 'Employment Date', 'Synergy Team']] #staff['Termination_Date'].nunique() was 0, so we don't include it\n",
    "staff['Employment Date'] = pd.to_datetime(staff['Employment Date'])\n",
    "staff.columns = staff.columns.str.replace(' ', '_')\n",
    "staff.head(1)\n",
    "len(staff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d9b54",
   "metadata": {},
   "source": [
    "**2 features to engineer:**\n",
    "* Employment_Total_Months\n",
    "* Manager_Is_Recent (move it to 'Projects' table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4935cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment_Total_Months\n",
    "staff['Employment_Total_Months'] = ((datetime.now() - staff['Employment_Date']).astype('timedelta64[M]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd9e82dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manager_Is_Recent\n",
    "managers = projects[['Project_ID', 'Project_Manager', 'Project_Start_Date']]\n",
    "managers = managers.merge(staff,how='left', left_on='Project_Manager', right_on='Staff_Name')\n",
    "managers.drop(columns = ['Staff_Name'], inplace = True)\n",
    "managers['Months_Before_Project'] = (managers['Project_Start_Date'] - managers['Employment_Date']).astype('timedelta64[M]')\n",
    "\n",
    "Manager_Is_Recent = {}\n",
    "\n",
    "for months in managers['Months_Before_Project']:\n",
    "    if np.isnan(months) == True:\n",
    "        continue\n",
    "    else:\n",
    "        if months < 6:\n",
    "            Manager_Is_Recent[months] = True\n",
    "        else:\n",
    "            Manager_Is_Recent[months] = False\n",
    "        \n",
    "df_12 = pd.DataFrame([{'Months_Before_Project': months, 'Manager_Is_Recent': recent_status} for (months, recent_status) in Manager_Is_Recent.items()])\n",
    "\n",
    "managers = managers.merge(df_12, how ='left', left_on='Months_Before_Project', right_on='Months_Before_Project')\n",
    "managers.head(1)\n",
    "len(managers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a79c877c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9755"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managers.drop(columns = ['Project_Manager', 'Project_Start_Date', 'Synergy_Team'], inplace = True)\n",
    "managers = managers[['Project_ID', 'Manager_Is_Recent']]\n",
    "projects = projects.merge(managers, how ='left', left_on='Project_ID', right_on='Project_ID')\n",
    "projects.head(1)\n",
    "len(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312c905",
   "metadata": {},
   "source": [
    "### 1.7 <a class=\"anchor\" id=\"1_7\"></a> Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7660b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Project_ID    Country         Office                         Sector  \\\n",
      "0    367704.0  Australia        Whyalla                 Ports & Marine   \n",
      "1    367705.0  Australia  WGASA Pty Ltd    Civic & Education Buildings   \n",
      "2    367706.0  Australia  WGASA Pty Ltd    Civic & Education Buildings   \n",
      "3    367707.0  Australia  WGASA Pty Ltd  Commercial & Retail Buildings   \n",
      "4    367708.0  Australia  WGASA Pty Ltd    Civic & Education Buildings   \n",
      "\n",
      "   Project_Size_Sort_Order Is_Multi_Discipline_Project  \\\n",
      "0                      4.0                       False   \n",
      "1                      1.0                       False   \n",
      "2                      1.0                       False   \n",
      "3                      3.0                       False   \n",
      "4                      7.0                       False   \n",
      "\n",
      "  Is_First_Client_Project Default_Rate_Group  Perc_of_Stages_with_Fixed_Fee  \\\n",
      "0                    True           Standard                          100.0   \n",
      "1                    True           Standard                            NaN   \n",
      "2                    True           Standard                            NaN   \n",
      "3                    True           Standard                            NaN   \n",
      "4                    True           Standard                            NaN   \n",
      "\n",
      "  Project_Manager Manager_Is_Recent  Project_Director  Perc_of_Subcontractors  \\\n",
      "0     David McKay             False      Mark Gilbert                     0.0   \n",
      "1     Mario Macri               NaN  Geoff Wallbridge                     0.0   \n",
      "2     Mario Macri               NaN  Loreto Taglienti                     0.0   \n",
      "3     Mario Macri               NaN      Mark Gilbert                     0.0   \n",
      "4    Peter McBean               NaN      Peter McBean                     0.0   \n",
      "\n",
      "   Project_Duration_Weeks Is_Front_Loaded Delivered_on_Time  \\\n",
      "0                     4.0            True               NaN   \n",
      "1                     NaN           False               NaN   \n",
      "2                     NaN           False               NaN   \n",
      "3                     NaN           False               NaN   \n",
      "4                     NaN           False               NaN   \n",
      "\n",
      "   Fully_In_Lockdown  Partially_In_Lockdown Suffered_Data_Loss  \\\n",
      "0              False                  False              False   \n",
      "1              False                  False                NaN   \n",
      "2              False                  False                NaN   \n",
      "3              False                  False                NaN   \n",
      "4              False                  False                NaN   \n",
      "\n",
      "   Total_Data_Issues  \n",
      "0                2.0  \n",
      "1                2.0  \n",
      "2                0.0  \n",
      "3                1.0  \n",
      "4                3.0  \n"
     ]
    }
   ],
   "source": [
    "# Drop columns unnecessary for analysis and rearrange\n",
    "projects = projects[['Project_ID', 'Country', 'Office', 'Sector', 'Project_Size_Sort_Order',\n",
    "                     'Is_Multi_Discipline_Project', 'Is_First_Client_Project',\n",
    "                     'Default_Rate_Group', 'Perc_of_Stages_with_Fixed_Fee',\n",
    "                     'Project_Manager', 'Manager_Is_Recent', 'Project_Director', 'Perc_of_Subcontractors',\n",
    "                     'Project_Duration_Weeks', 'Is_Front_Loaded', 'Delivered_on_Time',\n",
    "                     'Fully_In_Lockdown','Partially_In_Lockdown',\n",
    "                     'Suffered_Data_Loss', 'Total_Data_Issues']]\n",
    "\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14b11eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_projects\t api_clients\t api_projects\t checker\t clients\t custom_fields\t dates_prep\t df_1\t df_10\t \n",
      "df_11\t df_12\t df_2\t df_3\t df_4\t df_5\t df_6\t df_7\t df_8\t \n",
      "df_9\t external_projects\t first_half\t fixed_fee\t health\t issues\t managers\t pbi_clients\t pbi_projects\t \n",
      "project_dates\t projects\t staff\t stages\t subs\t successful_projects\t total_hours\t total_units\t transactions\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "753103a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release all dataframes from Python memory apart from final ones that go into the WGA schema\n",
    "dfs = [all_projects, api_clients, api_projects, checker, custom_fields, dates_prep, df_1, df_10, df_11, df_12,\n",
    "       df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, external_projects, first_half, fixed_fee, health, issues,\n",
    "       managers, pbi_clients, pbi_projects, project_dates, subs, successful_projects, total_hours,\n",
    "       total_units]\n",
    "del all_projects, api_clients, api_projects, checker, custom_fields, dates_prep, df_1, df_10, df_11, df_12, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, external_projects, first_half, fixed_fee, health, issues, managers, pbi_clients, pbi_projects, project_dates, subs, successful_projects, total_hours,total_units\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34c38825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clients\t projects\t staff\t stages\t transactions\t \n"
     ]
    }
   ],
   "source": [
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffacda",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the dataframes in Parquet format\n",
    "\n",
    "#projects.to_parquet('parquet-files/projects.parquet', index=False)\n",
    "#clients.to_parquet('parquet-files/clients.parquet', index=False)\n",
    "#stages.to_parquet('parquet-files/stages.parquet', index=False)\n",
    "#transactions.to_parquet('parquet-files/transactions.parquet', index=False)\n",
    "#staff.to_parquet('parquet-files/staff.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351a7e6",
   "metadata": {},
   "source": [
    "## Part 2: <a class=\"anchor\" id=\"part2\"></a> Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e856ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"JAVA_HOME\"] = '/Library/Java/JavaVirtualMachines/temurin-11.jdk/Contents/Home'\n",
    "os.environ[\"SPARK_HOME\"] = '/Users/MarfaPopova/S2R\\ Analytics/Development\\ &\\ Support\\ Team\\ -\\ Power\\ BI\\ for\\ Synergy\\ -\\ Advanced\\ Analytics/DataFlowExtract/venv/spark-3.2.1-bin-hadoop3.2.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91128583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '/content/spark-3.2.1-bin-hadoop3.2'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.1.2-bin-hadoop3.2\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\venv\\lib\\site-packages\\findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=157'>158</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=158'>159</a>\u001b[0m     py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(spark_python, \u001b[39m\"\u001b[39;49m\u001b[39mlib\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpy4j-*.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=159'>160</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\ETL\\full_etl_pipeline.ipynb Cell 64'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/ETL/full_etl_pipeline.ipynb#ch0000073?line=16'>17</a>\u001b[0m \u001b[39m# Import findspark lib \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/ETL/full_etl_pipeline.ipynb#ch0000073?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/ETL/full_etl_pipeline.ipynb#ch0000073?line=18'>19</a>\u001b[0m findspark\u001b[39m.\u001b[39;49minit()\n",
      "File \u001b[1;32mc:\\Users\\MarfaPopova\\S2R Analytics\\Development & Support Team - Power BI for Synergy - Advanced Analytics\\DataFlowExtract\\venv\\lib\\site-packages\\findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=158'>159</a>\u001b[0m         py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(spark_python, \u001b[39m\"\u001b[39m\u001b[39mlib\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpy4j-*.zip\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=159'>160</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=160'>161</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=161'>162</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to find py4j in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=162'>163</a>\u001b[0m                 spark_python\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=163'>164</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=164'>165</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=165'>166</a>\u001b[0m     sys\u001b[39m.\u001b[39mpath[:\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m sys_path \u001b[39m=\u001b[39m [spark_python, py4j]\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=166'>167</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/MarfaPopova/S2R%20Analytics/Development%20%26%20Support%20Team%20-%20Power%20BI%20for%20Synergy%20-%20Advanced%20Analytics/DataFlowExtract/venv/lib/site-packages/findspark.py?line=167'>168</a>\u001b[0m     \u001b[39m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.1.2-bin-hadoop3.2\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "# Install java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Last version of Spar\n",
    "!wget -q https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "\n",
    "# Unzip spark\n",
    "!tar xf /content/spark-3.2.1-bin-hadoop3.2\n",
    "\n",
    "# Setting environment variable\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "\n",
    "# Install findspark lib that help find spark in the system and import it as a regular lib\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
    "\n",
    "# Import findspark lib \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e728fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = sqlContext.read.parquet('parguet-files/projects.parquet', header=True)\n",
    "clients  = sqlContext.read.parquet('parguet-files/clients.parquet', header=True)\n",
    "stages = sqlContext.read.parquet('parguet-files/stages.parquet', header=True)\n",
    "transactions  = sqlContext.read.parquet('parguet-files/transactions.parquet', header=True)\n",
    "staff = sqlContext.read.parquet('parguet-files/staff.parquet', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_uri = \"jdbc:postgresql://opensea.c5pkb2dzarva.us-west-2.rds.amazonaws.com:5432/opensea\"\n",
    "user = \"marfapopova21\"\n",
    "password = \"qwerty123\"\n",
    "\n",
    "# Create a database and connect to it\n",
    "conn = sqlite3.connect('WGA.db')\n",
    "\n",
    "# Connecting to a database created in MS SQL Server Management Studio\n",
    "import pyodbc\n",
    "\n",
    "server = '.\\sqlexpress' \n",
    "database = 'wga' \n",
    "username = 'sa'  \n",
    "password  = 'marfa'\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+password)\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "# Test query\n",
    "sql_statement = \"select 1\"\n",
    "response = cursor.execute(sql_statement).fetchone()\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a024b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_showroom.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"dbtable\", \"nfts.assets\") \\\n",
    "    .option(\"url\", postgres_uri) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()\n",
    "\n",
    "assets_API.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", postgres_uri) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"dbtable\", \"nfts.assets\") \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()\n",
    "\n",
    "assets_ws.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"dbtable\", \"nfts.assets\") \\\n",
    "    .option(\"url\", postgres_uri) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()\n",
    "\n",
    "collections.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"url\", postgres_uri) \\\n",
    "    .option(\"dbtable\", \"nfts.collections\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()\n",
    "\n",
    "finances.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"dbtable\", \"nfts.finances\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"url\", postgres_uri) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()\n",
    "\n",
    "socials.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"url\", postgres_uri) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"dbtable\", \"nfts.socials\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()\n",
    "\n",
    "urls.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"dbtable\", \"nfts.urls\") \\\n",
    "    .option(\"url\", postgres_uri) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MSIN0097: Predictive Analytics.ipynb",
   "provenance": [
    {
     "file_id": "1wMnYS6Zd-TSClglZnpa6s87vKhDqcJPV",
     "timestamp": 1646991414543
    }
   ]
  },
  "interpreter": {
   "hash": "32faf87829e52f10b3379fa51fb017496aba8a2082e84bf41be67a5b199752f4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
