{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f4ab4c",
   "metadata": {
    "id": "57f4ab4c"
   },
   "source": [
    "<h1 align=\"center\">MSIN0114: Business Analytics Consulting Project</h1>\n",
    "<h2 align=\"center\">S2R Analytics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892fa8b",
   "metadata": {
    "id": "e892fa8b"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "**Data enginering (ETL pipeline)**\n",
    "\n",
    "* [Part 0](#part0): Data extraction\n",
    "\n",
    "* [Part 1](#part1): Data transformation\n",
    "    * [1.1](#1_1): Projects\n",
    "    * [1.2](#1_2): Transactions\n",
    "    * [1.3](#1_3): Stages\n",
    "    * [1.4](#1_4): Data health\n",
    "    * [1.5](#1_5): Clients \n",
    "    * [1.6](#1_6): Staff\n",
    "    * [1.7](#1_7): Clean-up    \n",
    " <br />\n",
    " \n",
    "* [Part 2](#part2): Data loading\n",
    "    * [2.1](#2_1): Database design and storage\n",
    "    * [2.2](#2_2): Conversion to flat file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F8qq-kkSaAp1",
   "metadata": {
    "id": "F8qq-kkSaAp1"
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BpTEkpFbb0yS",
   "metadata": {
    "id": "BpTEkpFbb0yS"
   },
   "outputs": [],
   "source": [
    "#Essentials\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from pandas.api.types import CategoricalDtype\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np; np.random.seed(2022)\n",
    "import random\n",
    "import sqlite3\n",
    "import pyodbc\n",
    "\n",
    "#Image creation and display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import pyplot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#from image import image, display\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#Other\n",
    "import itertools as it\n",
    "import io\n",
    "import os\n",
    "os.sys.path\n",
    "import sys\n",
    "import glob\n",
    "import concurrent.futures\n",
    "from __future__ import print_function\n",
    "import binascii\n",
    "import struct\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import scipy.cluster\n",
    "import datetime, time\n",
    "import functools, operator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8712833",
   "metadata": {},
   "source": [
    "## Part 0: <a class=\"anchor\" id=\"part0\"></a> Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65fe96",
   "metadata": {},
   "source": [
    "Main data is from API scripts from Jonny.\n",
    "X columns from PowerBI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb1179",
   "metadata": {
    "id": "7cdb1179"
   },
   "source": [
    "## Part 1: <a class=\"anchor\" id=\"part1\"></a> Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c2673",
   "metadata": {},
   "source": [
    "### 1.1 <a class=\"anchor\" id=\"1_1\"></a> Projects (wga.projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407addc",
   "metadata": {},
   "source": [
    "Step 1: Create a list of projects to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc6c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read all projects from Synergy API\n",
    "all_projects = pd.read_csv('csv-files/wga_synergy_incremental_projects.csv')\n",
    "all_projects = all_projects[['Project ID', 'Project Number', 'Project Name', 'Is Office Project', 'Is Billable', 'Project Status']]\n",
    "\n",
    "# Projects to keep: external (i.e. client only)\n",
    "external_projects = all_projects[(all_projects['Is Office Project'] != 'Yes')]\n",
    "external_projects = external_projects[(external_projects['Is Billable'] != 'No')]\n",
    "external_ids = external_projects['Project ID'].tolist()\n",
    "\n",
    "# Projects to keep: status-based\n",
    "successful_projects = external_projects[external_projects['Project Status'].isin(['Complete', 'Active', 'Pending Invoice']) == True]\n",
    "valid_ids = successful_projects['Project ID'].tolist()\n",
    "\n",
    "# See how many unique projects we shold have\n",
    "print('We should have ' + str(len(valid_ids)) + ' projects in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef2789f",
   "metadata": {},
   "source": [
    "Step 2: Cleaning data from Synergy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only valid projects\n",
    "api_projects = pd.read_csv('csv-files/wga_synergy_incremental_projects.csv')\n",
    "api_projects = (api_projects[api_projects['Project ID'].isin(valid_ids)])\n",
    "\n",
    "\n",
    "# Drop unnecesary columns\n",
    "api_projects.drop(columns = ['Unnamed: 0', 'Primary Contact Name', 'Status Name', 'Organisation ID',\n",
    "                             'customFields', 'Address Line 1', 'Address Line 2', 'Project Type ID',\n",
    "                             'Primary Contact', 'Primary Contact ID', 'Project Scope', 'Address Postal Code',\n",
    "                             'Address State', 'Address Town', 'Address Google', 'Client Reference Number',\n",
    "                             'Address State Postal Code Country', 'Address Single Line', 'Project Type Code',\n",
    "                             'External Name', 'Address Longitude', 'Address Latitude',\n",
    "                             'Project Forecast Value', 'Created Date', 'Updated Date', 'Manager ID'], inplace = True)\n",
    "\n",
    "\n",
    "# Convert columns for unified style\n",
    "api_projects.rename(columns = {'Invoices':'Number of Invoices', 'Project Net Residual (Neg as Zero)':'Project Net Residual',\n",
    "                              'Start Date (Project)': 'Project Start Date', 'End Date (Project)': 'Project End Date',\n",
    "                              'Address Country':'Country', 'Project Type': 'Sector'}, inplace = True)\n",
    "api_projects['Country'].replace(['AUSTRALIA', 'AUS', 'Autralia', 'NZ', 'new zealand', 'PNG', 'samoa', 'SAMOA', 'TONGA', 'SA', 'CHINA'],\n",
    "                                ['Australia', 'Australia', 'Australia', 'New Zealand', 'New Zealand', 'Papua New Guinea', 'Samoa', 'Samoa', 'Tonga', 'Saudi Arabia', 'China'],inplace=True)\n",
    "api_projects['Project Start Date'] = pd.to_datetime(api_projects['Project Start Date'])\n",
    "api_projects['Project End Date'] = pd.to_datetime(api_projects['Project End Date'])\n",
    "\n",
    "\n",
    "# Generalise minority observations into bigger groups\n",
    "api_projects['Sector'].mask(api_projects['Sector'] == 'Commercial', 'Commercial & Retail Buildings', inplace=True)\n",
    "api_projects['Sector'].mask(api_projects['Sector'] == 'Residential', 'Civic & Education Buildings', inplace=True)\n",
    "api_projects['Default Rate Group'].mask(api_projects['Default Rate Group'] != 'Standard', 'Non-standard', inplace=True)\n",
    "\n",
    "\n",
    "# Adding 'Due Date' and'Project Director' columns\n",
    "custom_fields = pd.read_csv('csv-files/wga_synergy_incremental_projects_custom_fields.csv')\n",
    "custom_fields = custom_fields[['PROPOSAL - Due Date', 'PROSPECT - Project Director', 'Project ID']].copy()\n",
    "custom_fields.rename(columns = {'PROSPECT - Project Director':'Project Director', 'PROPOSAL - Due Date': 'Due Date'}, inplace = True)\n",
    "custom_fields['Due Date'] = pd.to_datetime(custom_fields['Due Date'])\n",
    "api_projects = pd.merge(api_projects, custom_fields,  how='left', left_on='Project ID', right_on='Project ID')\n",
    "\n",
    "\n",
    "# Rearrange column names for easier interpretation\n",
    "api_projects = api_projects[['Project ID', 'Country',\n",
    "                             'Project Status', 'Sector',\n",
    "                             'Project Director', 'Project Manager', 'Office',\n",
    "                             'Project Start Date', 'Project End Date', 'Due Date',\n",
    "                             'Default Rate Group']]\n",
    "\n",
    "api_projects.head(1)\n",
    "len(api_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803bdc65",
   "metadata": {},
   "source": [
    "Step 3: Cleaning transformed PowerBI data from S2R Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0085b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pre-transformed data from PowerBI\n",
    "pbi_projects = pd.read_csv('csv-files/wga_power_bi_projects.csv', encoding = 'ISO-8859-1')\n",
    "pbi_projects = pbi_projects[['Project ID', 'Project Size Sort Order',\n",
    "                             'Project Duration (Weeks)', 'Is Multi Discipline Project','Is First Client Project']]\n",
    "\n",
    "# Load only valid projects\n",
    "pbi_projects = (pbi_projects[pbi_projects['Project ID'].isin(valid_ids)])\n",
    "\n",
    "# Convert columns for unified style\n",
    "pbi_projects.rename(columns = {'Project Duration (Weeks)':'Project Duration Weeks'}, inplace = True)\n",
    "pbi_projects['Is Multi Discipline Project'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "pbi_projects['Is First Client Project'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "\n",
    "pbi_projects.head(1)\n",
    "len(pbi_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0dd8b",
   "metadata": {},
   "source": [
    "Step 4: Merge the two 'Projects' tables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aeb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the projects table from API and preprocesed Power BI table\n",
    "projects = pd.merge(api_projects, pbi_projects,  how='left', left_on='Project ID', right_on='Project ID')\n",
    "projects.columns = projects.columns.str.replace(' ', '_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e67c2",
   "metadata": {},
   "source": [
    "**4 features to engineer:**\n",
    "* Delivered_on_Time\n",
    "* Fully_In_Lockdown\n",
    "* Partially_In_Lockdown\n",
    "* Suffered_Data_Loss (projects that started after July 2018 did not suffer from data loss, projects that ended before July 2018 did not suffer from data loss)\n",
    "\n",
    "The features are engineered after dealing with transactions table to fill in blanks of missing start and end dates of projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9d4d6",
   "metadata": {},
   "source": [
    "### 1.2 <a class=\"anchor\" id=\"1_2\"></a> Transactions (wga.transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only valid projects' transactions from Synergy API.\n",
    "transactions = pd.read_csv('csv-files/wga_sql_transactions.csv')\n",
    "transactions = (transactions[transactions['projectId'].isin(valid_ids)])\n",
    "\n",
    "transactions = transactions[['id', 'projectId', 'stageId', 'transactionTypeId',\n",
    "                             'rateType', 'status','units','valueTotal',\n",
    "                             'invoiceValueTotal','actualCostTotal',\n",
    "                             'targetChargeTotal', 'date']]\n",
    "\n",
    "transactions.rename(columns = {'id':'Transaction ID', 'projectId':'Project ID',\n",
    "                               'transactionTypeId': 'Transaction Type',\n",
    "                               'rateType': 'Rate Type', 'status': 'Status',\n",
    "                               'stageId': 'Stage ID', 'date':'Date',\n",
    "                               'invoiceValueTotal': 'Invoice Value Total',\n",
    "                               'actualCostTotal':'Actual Cost Total',\n",
    "                               'targetChargeTotal':'Target Charge Total',\n",
    "                               'valueTotal':'Value Total',\n",
    "                               'units': 'Units'}, inplace = True)\n",
    "\n",
    "transactions = transactions[(transactions['Status'] == 'Invoiced') | (transactions['Status'] == 'Written off')]\n",
    "transactions['Transaction Type'].replace([100, 200, 300, 400, 500, 700, 750, 800],\n",
    "                                         ['Time', 'Cash', 'Travel', 'Office', 'Bill', 'Balance', 'Unearned', 'Invoice Custom'], inplace=True)\n",
    "transactions['Date'] = pd.to_datetime(transactions['Date'])\n",
    "transactions.columns = transactions.columns.str.replace(' ', '_')\n",
    "\n",
    "transactions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e6bf6",
   "metadata": {},
   "source": [
    "**4 features to engineer:**\n",
    "* Perc_of_Subcontractors (move to 'Projects' table)\n",
    "* Is_Front_Loaded (move to 'Projects' table)\n",
    "* Avg_Profit (move to 'Projects' table) - average profit margin per project\n",
    "* Avg_Rec (move to 'Projects' table) - average financial recoverability per project\n",
    "\n",
    "**2 features to update:**\n",
    "* Project_Start_Date, Project_End_Date for projects with absent dates in the 'projects' table.\n",
    "\n",
    "**Table alterations:**\n",
    "* FK Time_Profile (links 'wga.projects' table on 'Project_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f03e84",
   "metadata": {},
   "source": [
    "Perc_of_Subcontractors = \n",
    "* total units of subcontractors divided by\n",
    "* sum of units where transaction type is 'bill' or 'time'\n",
    "\n",
    "* 'Time' = Company's employees\n",
    "* 'Bill' = Hired subcontrators\n",
    "* Time + Bill = total human capital on project in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perc_of_Subcontractors\n",
    "subs = transactions[['Project_ID', 'Units', 'Rate_Type']]\n",
    "subs = subs[(subs['Rate_Type'] == 'Subcontractor')]\n",
    "subs.drop(columns = ['Rate_Type'], inplace = True)\n",
    "subs = pd.DataFrame(subs.groupby(['Project_ID'])['Units'].count()).reset_index()\n",
    "subs.rename(columns = {'Units': 'Sub_Hours_Per_Project'}, inplace = True)\n",
    "\n",
    "total_hours = transactions[['Project_ID', 'Units', 'Transaction_Type']]\n",
    "total_hours = total_hours[(total_hours['Transaction_Type'] == 'Time') | (total_hours['Transaction_Type'] == 'Bill')]\n",
    "total_hours = pd.DataFrame(total_hours.groupby(['Project_ID'])['Units'].count()).reset_index()\n",
    "total_hours.rename(columns = {'Units': 'Total_Hours_Per_Project'}, inplace = True)\n",
    "\n",
    "df_1 = pd.merge(projects, subs, how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "df_2 = pd.merge(df_1, total_hours, how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "df_2['Sub_Hours_Per_Project'].fillna(0, inplace=True)\n",
    "df_2['Total_Hours_Per_Project'].fillna(0, inplace=True)\n",
    "df_2['Perc_of_Subcontractors'] = (df_2['Sub_Hours_Per_Project'] / df_2['Total_Hours_Per_Project']).round(decimals = 2)\n",
    "df_2 = df_2[['Project_ID', 'Perc_of_Subcontractors']]\n",
    "\n",
    "# Add the new feature to the 'Projects' table\n",
    "projects = pd.merge(projects, df_2,  how='left', left_on='Project_ID', right_on='Project_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfdf424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is_Front_Loaded\n",
    "project_dates = projects[['Project_ID', 'Project_Start_Date', 'Project_End_Date']]\n",
    "df_3 = transactions[['Project_ID', 'Units', 'Date']]\n",
    "df_3 = pd.merge(df_3, project_dates, how='left', left_on='Project_ID', right_on='Project_ID')\n",
    "\n",
    "first_half = df_3[(df_3['Date']  < df_3['Project_Start_Date'] + (df_3['Project_End_Date'] - df_3['Project_Start_Date'])/2)] # finding mid-point between 2 dates\n",
    "first_half = pd.DataFrame(first_half.groupby(['Project_ID'])['Units'].sum()).reset_index()\n",
    "first_half.rename(columns = {'Units': '1st_Half_Units'}, inplace = True)\n",
    "\n",
    "total_units = pd.DataFrame(df_3.groupby(['Project_ID'])['Units'].sum()).reset_index()\n",
    "total_units.rename(columns = {'Units': 'Total_Effort_Units'}, inplace = True)\n",
    "\n",
    "df_4 = pd.merge(total_units, first_half, how ='left', left_on='Project_ID', right_on='Project_ID')\n",
    "df_4['Perc_Being_Front'] = df_4['1st_Half_Units']/df_4['Total_Effort_Units']\n",
    "df_4['Is_Front_Loaded'] = (df_4['Perc_Being_Front']>=0.7)\n",
    "df_4 = df_4[['Project_ID', 'Is_Front_Loaded']]\n",
    "\n",
    "# Add the new feature to the 'Projects' table\n",
    "projects = pd.merge(projects, df_4,  how='left', left_on='Project_ID', right_on='Project_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recoverability, Profit_Measure\n",
    "transactions = transactions[['Project_ID', 'Stage_ID', 'Value_Total', 'Invoice_Value_Total', 'Actual_Cost_Total', 'Target_Charge_Total', 'Date']]\n",
    "transactions['Recoverability'] = transactions['Value_Total']/transactions['Target_Charge_Total']\n",
    "infinites = transactions[(transactions['Recoverability'] == np.inf) | (transactions['Recoverability'] == -np.inf)]\n",
    "#infinites['Target_Charge_Total'].sum() - shows that all cells in 'Target_Charge_Total' column are equal to 0, creating unwanted infinite values\n",
    "\n",
    "transactions = transactions[(transactions['Target_Charge_Total'] != 0)]\n",
    "#transactions['Recoverability'].min(), transactions['Recoverability'].max() - shows that there limits are real numbers, not infinite\n",
    "\n",
    "transactions['Profit_Measure'] = transactions['Invoice_Value_Total']/transactions['Actual_Cost_Total']\n",
    "#transactions['Profit_Measure'].min(), transactions['Profit_Measure'].max()  - shows that there limits are real numbers, not infinite\n",
    "transactions = transactions[['Project_ID', 'Stage_ID', 'Date', 'Recoverability', 'Profit_Measure']]\n",
    "\n",
    "# stage_transactions\n",
    "df_5 = pd.DataFrame(transactions.groupby(['Project_ID', 'Stage_ID'])['Recoverability'].count()).reset_index()\n",
    "df_5.rename(columns = {'Recoverability':'Count'}, inplace = True)\n",
    "stage_transactions = pd.DataFrame(transactions.groupby(['Project_ID', 'Stage_ID'])['Recoverability', 'Profit_Measure'].sum()).reset_index()\n",
    "stage_transactions = pd.merge(stage_transactions, df_5, how='left', on=['Project_ID', 'Stage_ID'])\n",
    "stage_transactions['Avg_Rec'] = stage_transactions['Recoverability']/stage_transactions['Count']\n",
    "stage_transactions['Avg_Profit'] = stage_transactions['Profit_Measure']/stage_transactions['Count']\n",
    "stage_transactions = stage_transactions[['Project_ID', 'Stage_ID', 'Avg_Rec', 'Avg_Profit']]\n",
    "\n",
    "# project_transactions\n",
    "df_6 =  pd.DataFrame(transactions.groupby(['Project_ID'])['Recoverability'].count()).reset_index()\n",
    "df_6.rename(columns = {'Recoverability':'Count'}, inplace = True)\n",
    "project_transactions =  pd.DataFrame(transactions.groupby(['Project_ID'])['Recoverability', 'Profit_Measure'].sum()).reset_index()\n",
    "project_transactions = pd.merge(project_transactions, df_6, how='left', on='Project_ID')\n",
    "project_transactions['Avg_Rec'] = project_transactions['Recoverability']/project_transactions['Count']\n",
    "project_transactions['Avg_Profit'] = project_transactions['Profit_Measure']/project_transactions['Count']\n",
    "project_transactions = project_transactions[['Project_ID', 'Avg_Rec', 'Avg_Profit']]\n",
    "\n",
    "# Add the 2 new features to the 'Projects' table\n",
    "projects = pd.merge(projects, project_transactions, how='left', on='Project_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e96e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Only ' + str(transactions['Project_ID'].nunique()) + ' projects have transaction recorded, meaning ' + str(len(projects) - transactions['Project_ID'].nunique()) + ' projects will be missing from transaction tables.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc80800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project_Start_Date, Project_End_Date\n",
    "\n",
    "def nat_check(date):\n",
    "    if type(date) == pd._libs.tslibs.nattype.NaTType:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "min_dates =  pd.DataFrame(transactions.groupby(['Project_ID'])['Date'].min()).reset_index()\n",
    "min_dates.rename(columns = {'Date':'Min_Date'}, inplace = True)\n",
    "max_dates =  pd.DataFrame(transactions.groupby(['Project_ID'])['Date'].max()).reset_index()\n",
    "max_dates.rename(columns = {'Date':'Max_Date'}, inplace = True)\n",
    "all_dates = pd.merge(min_dates, max_dates, how='left', on='Project_ID')\n",
    "projects = pd.merge(projects, all_dates, how='left', on='Project_ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dca736",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects['Project_Start_Date'] = projects['Project_Start_Date'].map(str)\n",
    "projects['Project_End_Date'] = projects['Project_End_Date'].map(str)\n",
    "projects['Due_Date'] = projects['Due_Date'].map(str)\n",
    "\n",
    "projects.loc[projects['Project_Start_Date']=='NaT','Project_Start_Date']=projects['Min_Date']\n",
    "projects.loc[projects['Project_End_Date']=='NaT','Project_End_Date']=projects['Max_Date']\n",
    "projects.loc[projects['Due_Date']=='NaT','Due_Date']=projects['Max_Date']\n",
    "\n",
    "projects['Project_Start_Date'] = pd.to_datetime(projects['Project_Start_Date'])\n",
    "projects['Project_End_Date'] = pd.to_datetime(projects['Project_End_Date'])\n",
    "projects['Due_Date'] = pd.to_datetime(projects['Due_Date'])\n",
    "\n",
    "projects.drop(columns = ['Min_Date', 'Max_Date'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189cddf",
   "metadata": {},
   "source": [
    "Now, let's go back to engieering date-dependent features with newly filled in values in the 'Projects' table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delivered_on_Time\n",
    "    \n",
    "Delivered_on_Time = {}\n",
    "\n",
    "for due_date in projects['Due_Date']:\n",
    "    for completed in projects['Project_End_Date']:\n",
    "        if nat_check(due_date) == True:\n",
    "            continue\n",
    "        else:\n",
    "            if due_date <= completed:\n",
    "                Delivered_on_Time[due_date] = True\n",
    "            else:\n",
    "                Delivered_on_Time[due_date] = False\n",
    "\n",
    "df_7 = pd.DataFrame([{'Due_Date': due_date, 'Delivered_on_Time': is_on_time} for (due_date, is_on_time) in Delivered_on_Time.items()])\n",
    "\n",
    "projects = pd.merge(projects, df_7, how='left', on='Due_Date')\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully_In_Lockdown, Partially_In_Lockdown\n",
    "\n",
    "Lockdown_Period = (pd.date_range(start='2020-03-16', end = '2020-11-21', freq='D')).to_series()\n",
    "\n",
    "projects['Start_in_Lockdown'] = projects['Project_Start_Date'].isin([Lockdown_Period])\n",
    "projects['End_in_Lockdown'] = projects['Project_End_Date'].isin([Lockdown_Period])\n",
    "dates_prep = pd.concat([projects['Start_in_Lockdown'], projects['End_in_Lockdown']], axis = 1) #axis=1 specifies horizontal stacking\n",
    "\n",
    "projects['Fully_In_Lockdown'] = pd.DataFrame(dates_prep.all(axis=1))\n",
    "projects['Partially_In_Lockdown'] = pd.DataFrame(dates_prep.any(axis=1))\n",
    "\n",
    "projects.drop(columns = ['Start_in_Lockdown', 'End_in_Lockdown'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffered_Data_Loss\n",
    "\n",
    "#def data_loss_check(start_date, end_date):\n",
    "#    if start_date < pd.Timestamp('2018-07-15') and end_date < pd.Timestamp('2018-07-15'): #project started and ended before the acqusition\n",
    "#        return False\n",
    "#    elif start_date > pd.Timestamp('2018-07-15') and end_date > pd.Timestamp('2018-07-15'): #project started and ended after the acqusition\n",
    "#        return False\n",
    "#    elif start_date < pd.Timestamp('2018-07-15') and end_date > pd.Timestamp('2018-07-15'): #project started before the acqusition but ended after it\n",
    "#        return True\n",
    "\n",
    "#Data_Loss_Check = {}\n",
    "\n",
    "#for start_date in projects['Project_Start_Date']:\n",
    "#    for end_date in projects['Project_End_Date']:\n",
    "#        if (nat_check(start_date) or nat_check(end_date)) == True:\n",
    "#            continue\n",
    "#        else:\n",
    "#            if data_loss_check(start_date, end_date) == True:\n",
    "#                Data_Loss_Check[start_date.strftime(format = '%Y-%m-%d %H:%M:%S'), end_date.strftime(format = '%Y-%m-%d %H:%M:%S')] = True\n",
    "#            else:\n",
    "#                Data_Loss_Check[start_date.strftime(format = '%Y-%m-%d %H:%M:%S'), end_date.strftime(format = '%Y-%m-%d %H:%M:%S')] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL: https://www.geeksforgeeks.org/python-program-to-convert-a-tuple-to-a-string/#:~:text=There%20are%20various%20approaches%20to%20convert%20a%20tuple,of%20the%20tuple%20and%20convert%20it%20into%20string.\n",
    "\n",
    "#def convertTuple(tup):\n",
    "#    string = ', '.join(tup)\n",
    "#    return string\n",
    "\n",
    "#Execution_Timeframe = Data_Loss_Check.copy()\n",
    "\n",
    "#for key in Execution_Timeframe.keys():\n",
    "#    Execution_Timeframe[key] = convertTuple(key)\n",
    "    \n",
    "# Changing keys of our final dictionary\n",
    "#Suffered_Data_Loss = dict(zip((Execution_Timeframe.values()), (Data_Loss_Check.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed299ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column in 'Projects' table to create merge on\n",
    "#projects['Execution_Timeframe'] = projects['Project_Start_Date'].map(str) + ', ' + projects['Project_End_Date'].map(str)\n",
    "#projects['Execution_Timeframe'][0]\n",
    "#type(projects['Execution_Timeframe'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the two future columns will merge\n",
    "#list(Suffered_Data_Loss)[0] == projects['Execution_Timeframe'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_8 = pd.DataFrame.from_dict(Suffered_Data_Loss, orient ='index')\n",
    "#df_8 = df_8.reset_index()\n",
    "#df_8.rename(columns = {'index':'Execution_Timeframe', 0:'Suffered_Data_Loss'}, inplace = True)\n",
    "\n",
    "#projects = pd.merge(projects, df_8, how='left', on='Execution_Timeframe')\n",
    "#projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9cee98",
   "metadata": {},
   "source": [
    "### 1.3 <a class=\"anchor\" id=\"1_3\"></a> Stages (wga.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only valid projects' stages\n",
    "stages = pd.read_csv('csv-files/wga_power_bi_stages.csv', encoding = 'ISO-8859-1')\n",
    "stages = (stages[stages['Project ID'].isin(valid_ids)])\n",
    "stages = stages[(stages['Stage Type'] != 'Proposal')] # We only want professional fees\n",
    "stages = stages[['Project ID', 'Stage ID', 'Stage Fee Type', 'Is Disbursement Stage',\n",
    "                 'Stage Manager', 'Stage Discipline','Stage Start Date','Stage End Date']]\n",
    "\n",
    "stages['Is Disbursement Stage'].replace(['No', 'Yes'], [False, True],inplace=True)\n",
    "stages['Stage Start Date'] = pd.to_datetime(stages['Stage Start Date'])\n",
    "stages['Stage End Date'] = pd.to_datetime(stages['Stage End Date'])\n",
    "stages.columns = stages.columns.str.replace(' ', '_')\n",
    "\n",
    "# Add profit and recoverability measures to the 'Stages' table\n",
    "stages = pd.merge(stages, stage_transactions, how='left', left_on=['Project_ID', 'Stage_ID'], right_on = ['Project_ID', 'Stage_ID'])\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698caac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages['Project_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fac5c9",
   "metadata": {},
   "source": [
    "**1 feature to engineer:**\n",
    "* Perc_of_Stages_with_Fixed_Fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59292886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perc_of_Stages_with_Fixed_Fee\n",
    "df_9 = pd.DataFrame(stages.groupby(['Project_ID', 'Stage_Fee_Type'])['Stage_ID'].count()).reset_index()\n",
    "df_10 = pd.DataFrame(stages.groupby(['Project_ID'])['Stage_Fee_Type'].count()).reset_index()\n",
    "df_10.rename(columns = {'Stage_Fee_Type':'Total_Num_Stages'}, inplace = True)\n",
    "df_10 = pd.merge(df_9, df_10, how='left', on='Project_ID')\n",
    "df_10.rename(columns = {'Stage_ID':'Num_of_Stages_Per_Type'}, inplace = True)\n",
    "df_11 = df_10[(df_10['Stage_Fee_Type'] == 'Fixed fee')]\n",
    "df_11['Perc_of_Stages_with_Fixed_Fee'] = (df_11['Num_of_Stages_Per_Type'] / df_11['Total_Num_Stages']).round(decimals = 2)\n",
    "df_11 = df_11[['Project_ID', 'Perc_of_Stages_with_Fixed_Fee']]\n",
    "all_stages =  pd.merge(df_10, df_11,  how='left', on='Project_ID')\n",
    "all_stages = all_stages.fillna(0)\n",
    "test = pd.merge(stages, all_stages,  how='left', on='Project_ID')\n",
    "test['Perc_of_Stages_with_Fixed_Fee'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(test['Perc_of_Stages_with_Fixed_Fee'].isnull())\n",
    "nulls.rename(columns = {'Perc_of_Stages_with_Fixed_Fee':'checker'}, inplace = True)\n",
    "nulls = nulls.loc[nulls['checker'] == True]\n",
    "indexes = list(nulls.index.values)\n",
    "nulls = test.iloc[indexes]\n",
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the projects with mising values don't have fixed fee at all, let's fill them with zero\n",
    "test = test.fillna(0)\n",
    "test['Perc_of_Stages_with_Fixed_Fee'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb288b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['Project_ID', 'Total_Num_Stages', 'Perc_of_Stages_with_Fixed_Fee']]\n",
    "test.drop_duplicates(inplace = True, ignore_index=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = pd.merge(projects, test,  how ='left', on ='Project_ID')\n",
    "projects['Perc_of_Stages_with_Fixed_Fee'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = pd.DataFrame(projects['Perc_of_Stages_with_Fixed_Fee'].isnull())\n",
    "nulls.rename(columns = {'Perc_of_Stages_with_Fixed_Fee':'checker'}, inplace = True)\n",
    "nulls = nulls.loc[nulls['checker'] == True]\n",
    "indexes = list(nulls.index.values)\n",
    "nulls = projects.iloc[indexes]\n",
    "nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages.loc[stages['Project_ID'] == 15907 | 59885 | 60137]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7f91e",
   "metadata": {},
   "source": [
    "It turns out, 3 projects are not included in the 'stages' tables, so their stages attributes are not recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage_Duration_Weeks\n",
    "#df_4 = pd.DataFrame(stages['Stage_Start_Date'].notnull() & stages['Stage_End_Date'].notnull())\n",
    "#df_4.rename(columns = {0:'checker'}, inplace = True)\n",
    "#df_4 = df_4.loc[df_4['checker'] == True]\n",
    "\n",
    "#stages = pd.merge(stages, df_4, left_index=True, right_index=True)\n",
    "#stages['Stage_Duration_Weeks'] = ((stages['Stage_End_Date'] - stages['Stage_Start_Date']).astype('timedelta64[W]'))\n",
    "#stages.drop(columns = 'checker', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects['Fully_In_Lockdown'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ba4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects['Partially_In_Lockdown'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091de450",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects.drop(columns = ['Fully_In_Lockdown', 'Partially_In_Lockdown'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16897d55",
   "metadata": {},
   "source": [
    "### 1.4 <a class=\"anchor\" id=\"1_4\"></a> Data health (wga.health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e404917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only valid projects\n",
    "health = pd.read_csv('csv-files/wga_power_bi_stages.csv', encoding = 'ISO-8859-1')\n",
    "health = (health[health['Project ID'].isin(valid_ids)])\n",
    "\n",
    "\n",
    "# Only leave columns that are relevant\n",
    "health = health[['Project ID', 'Stage ID',\n",
    "                 'Data Quality - Has Issues',\n",
    "                 'Data Quality - Has Inactive Staff Resourced', \n",
    "                 'Data Quality - Rate Group', 'Health - % Duration Complete',\n",
    "                 'Health - % Fee Used', 'Health - Stages With Alerts #']]\n",
    "\n",
    "# Convert columns for unified style\n",
    "health.rename(columns = {'Data Quality - Has Issues': 'DQ_Has_Issues',\n",
    "                         'Data Quality - Has Inactive Staff Resourced':'DQ_Has_Inactive_Staff_Resourced',\n",
    "                         'Data Quality - Rate Group':'DQ_Rate_Group',\n",
    "                         'Health - % Duration Complete':'Health_Perc_Duration_Complete',\n",
    "                         'Health - % Fee Used':'Health_Perc_Fee_Used',\n",
    "                         'Health - Stages With Alerts #':'Alerts_Total_Per_Stage'}, inplace = True)\n",
    "\n",
    "health['DQ_Has_Issues'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "health['DQ_Has_Inactive_Staff_Resourced'].replace(['No', 'Yes'],[False, True],inplace=True)\n",
    "health.columns = health.columns.str.replace(' ', '_')\n",
    "\n",
    "health.head(1)\n",
    "len(health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = health[health['Project_ID'].isin([368035]) == True]\n",
    "checker = checker[['Project_ID', 'Stage_ID', 'Alerts_Total_Per_Stage']]\n",
    "checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d0550",
   "metadata": {},
   "source": [
    "**1 feature to engineer:**\n",
    "* Total_Data_Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alerts_Total_Per_Project\n",
    "issues = health.groupby(['Project_ID'], sort=False).sum('Alerts_Total_Per_Stage').reset_index()\n",
    "issues = issues[['Project_ID', 'Alerts_Total_Per_Stage']]\n",
    "issues.rename(columns = {'Alerts_Total_Per_Stage':'Total_Data_Issues'}, inplace = True)\n",
    "projects = pd.merge(projects, issues, how ='left', on='Project_ID')\n",
    "projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c19eaa",
   "metadata": {},
   "source": [
    "### 1.5 <a class=\"anchor\" id=\"1_5\"></a> Clients (wga.clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70055d",
   "metadata": {},
   "source": [
    "Step 1: Cleaning all given data, from Synergy API and Power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e69387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Cleaning data from Synergy API.\n",
    "api_clients = pd.read_csv('csv-files/wga_synergy_overnight_1_clients.csv')\n",
    "api_clients.drop(columns = {'Client Name', 'Unnamed: 0', 'Contact Type', 'Organisation ID'}, inplace = True)\n",
    "api_clients['Created Date'] = pd.to_datetime(api_clients['Created Date'])\n",
    "\n",
    "# Step 2: Cleaning transformed PowerBI data from S2R Analytics.\n",
    "pbi_clients = pd.read_csv('csv-files/wga_power_bi_clients.csv', encoding = 'ISO-8859-1')\n",
    "pbi_clients = pbi_clients[['Client ID', 'Client Projects - Total No', 'Client Projects - First Project ID']]\n",
    "pbi_clients.rename(columns = {'Client Projects - Total No': 'Client Projects Total No',\n",
    "                              'Client Projects - First Project ID':'1st Project ID'}, inplace = True)\n",
    "\n",
    "# Step 3: Merge the two 'Clients' tables together.\n",
    "clients = pd.merge(api_clients, pbi_clients,  how='left', left_on='Client ID', right_on='Client ID')\n",
    "clients.columns = clients.columns.str.replace(' ', '_')\n",
    "clients.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients['Client_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f115ff",
   "metadata": {},
   "source": [
    "**3 features to engineer:**\n",
    "* Client_Duration_Months\n",
    "* Client_Is_Repeated\n",
    "* Client_Is_Recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_Is_Repeated\n",
    "clients['Client_Is_Repeated'] = clients['1st_Project_ID'].notnull()\n",
    "\n",
    "# Client_Duration_Months\n",
    "clients['Client_Duration_Months'] = datetime.now() - clients['Created_Date']\n",
    "clients['Client_Duration_Months'] = (clients['Client_Duration_Months'].astype('timedelta64[M]'))\n",
    "clients['Client_Duration_Months'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852759cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client_Is_Recent\n",
    "\n",
    "Client_Is_Recent = {}\n",
    "\n",
    "for months in clients['Client_Duration_Months']:\n",
    "    if months < 6:\n",
    "        Client_Is_Recent[months] = True\n",
    "    else:\n",
    "        Client_Is_Recent[months] = False\n",
    "         \n",
    "df_12 = pd.DataFrame(\n",
    "    [{'Client_Duration_Months': months, 'Client_Is_Recent': recent_status} for (months, recent_status) in Client_Is_Recent.items()])\n",
    "\n",
    "clients = pd.merge(clients, df_12, how='left', on='Client_Duration_Months')\n",
    "#clients['1st_Project_ID'] = clients['1st_Project_ID'].astype(int)\n",
    "clients.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed40f7",
   "metadata": {},
   "source": [
    "### 1.6 <a class=\"anchor\" id=\"1_6\"></a> Human resources (wga.staff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "staff = pd.read_csv('csv-files/wga_synergy_overnight_1_staff.csv')\n",
    "staff = staff[['Staff ID', 'Staff Name', 'Employment Date', 'Synergy Team']] #staff['Termination_Date'].nunique() was 0, so we don't include it\n",
    "staff['Employment Date'] = pd.to_datetime(staff['Employment Date'])\n",
    "staff.columns = staff.columns.str.replace(' ', '_')\n",
    "staff.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d9b54",
   "metadata": {},
   "source": [
    "**2 features to engineer:**\n",
    "* Employment_Total_Months\n",
    "* Manager_Is_Recent (move it to 'Projects' table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment_Total_Months\n",
    "staff['Employment_Total_Months'] = ((datetime.now() - staff['Employment_Date']).astype('timedelta64[M]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manager_Is_Recent\n",
    "managers = projects[['Project_ID', 'Project_Manager', 'Project_Start_Date']]\n",
    "managers = pd.merge(managers, staff, how='left', left_on='Project_Manager', right_on='Staff_Name')\n",
    "managers.drop(columns = ['Staff_Name'], inplace = True)\n",
    "managers['Months_Before_Project'] = (managers['Project_Start_Date'] - managers['Employment_Date']).astype('timedelta64[M]')\n",
    "\n",
    "Manager_Is_Recent = {}\n",
    "\n",
    "for months in managers['Months_Before_Project']:\n",
    "    if np.isnan(months) == True:\n",
    "        continue\n",
    "    else:\n",
    "        if months < 6:\n",
    "            Manager_Is_Recent[months] = True\n",
    "        else:\n",
    "            Manager_Is_Recent[months] = False\n",
    "        \n",
    "df_13 = pd.DataFrame([{'Months_Before_Project': months, 'Manager_Is_Recent': recent_status} for (months, recent_status) in Manager_Is_Recent.items()])\n",
    "\n",
    "managers = pd.merge(managers, df_13, how ='left', on='Months_Before_Project')\n",
    "managers.head(1)\n",
    "len(managers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "managers.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "managers = managers[['Project_ID',\t'Staff_ID', 'Synergy_Team', 'Employment_Total_Months', 'Manager_Is_Recent']]\n",
    "projects = pd.merge(projects, managers, how ='left', on='Project_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312c905",
   "metadata": {},
   "source": [
    "### 1.7 <a class=\"anchor\" id=\"1_7\"></a> Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2b3da",
   "metadata": {},
   "source": [
    "Dropped 7 columns: Project_Status, Project_Start_Date,\tProject_End_Date,\tDue_Date, Number_of_Invoices,\tProject_Net_Residual, Execution_Timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns unnecessary for analysis and rearrange\n",
    "projects = projects[['Project_ID', 'Country', 'Office', 'Sector', 'Project_Size_Sort_Order',\n",
    "                     'Total_Num_Stages', 'Is_Multi_Discipline_Project', 'Is_First_Client_Project',\n",
    "                     'Default_Rate_Group', 'Perc_of_Stages_with_Fixed_Fee',\n",
    "                     'Project_Manager', 'Manager_Is_Recent', 'Project_Director', 'Perc_of_Subcontractors',\n",
    "                     'Project_Duration_Weeks', 'Is_Front_Loaded', 'Delivered_on_Time',\n",
    "                     'Total_Data_Issues', 'Avg_Rec', 'Avg_Profit']]\n",
    "\n",
    "projects.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b11eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753103a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release all dataframes from Python memory apart from final ones that go into the WGA schema\n",
    "dfs = [all_dates, all_projects, all_stages, api_clients, api_projects, checker, custom_fields, dates_prep, df_1, df_10, df_11, df_12, df_13,\n",
    "       df_2, df_3, df_4, df_5, df_6, df_7, df_9, external_projects, first_half, health, infinites, issues,\n",
    "       managers, max_dates, min_dates, nulls, pbi_clients, pbi_projects, project_dates, project_transactions, stage_transactions, subs,\n",
    "       successful_projects, test, total_hours, total_units]\n",
    "del all_dates, all_projects, all_stages, api_clients, api_projects, checker, custom_fields, dates_prep, df_1, df_10, df_11, df_12, df_13, df_2, df_3, df_4, df_5, df_6, df_7, df_9, external_projects, first_half, health, infinites, issues, managers,  max_dates, min_dates, nulls, pbi_clients, pbi_projects, project_dates, project_transactions, stage_transactions, subs, successful_projects, test, total_hours,total_units\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c38825",
   "metadata": {},
   "outputs": [],
   "source": [
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffacda",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the dataframes in Parquet format\n",
    "\n",
    "#projects.to_parquet('parquet-files/projects.parquet', index=False)\n",
    "#clients.to_parquet('parquet-files/clients.parquet', index=False)\n",
    "#stages.to_parquet('parquet-files/stages.parquet', index=False)\n",
    "#transactions.to_parquet('parquet-files/transactions.parquet', index=False)\n",
    "#staff.to_parquet('parquet-files/staff.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351a7e6",
   "metadata": {},
   "source": [
    "## Part 2: <a class=\"anchor\" id=\"part2\"></a> Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a9ece",
   "metadata": {},
   "source": [
    "### 2.1 <a class=\"anchor\" id=\"2_1\"></a> Database design and storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25224c",
   "metadata": {},
   "source": [
    "### 2.2 <a class=\"anchor\" id=\"2_2\"></a> Conversion to flat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat file on project level\n",
    "project_lvl = pd.merge(projects, staff, how='left', left_on='Project_Manager', right_on='Staff_Name')\n",
    "project_lvl.drop(columns = ['Staff_Name', 'Project_Manager', 'Employment_Date'], inplace = True)\n",
    "project_lvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat file on stage level\n",
    "\n",
    "stage_lvl = pd.merge(project_lvl, clients, how='left', left_on='Project_ID', right_on='1st_Project_ID')\n",
    "stage_lvl.drop(columns = ['1st_Project_ID', 'Created_Date'], inplace = True)\n",
    "stage_lvl.rename(columns = {'Staff_ID':'Project_Manager'}, inplace = True)\n",
    "stages_prep = stages[['Project_ID',  'Stage_ID', 'Is_Disbursement_Stage', 'Stage_Discipline', 'Stage_Fee_Type']]\n",
    "stage_lvl = pd.merge(stage_lvl, stages_prep, how='left', on='Project_ID')\n",
    "stage_lvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5261d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_lvl.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b25c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat file on transaction level\n",
    "transaction_lvl = pd.merge(stage_lvl, transactions, how='left', on=['Project_ID', 'Stage_ID'])\n",
    "\n",
    "transaction_lvl = transaction_lvl[['Project_ID', 'Country', 'Office', 'Sector', 'Project_Size_Sort_Order', 'Total_Num_Stages',\n",
    "'Is_Multi_Discipline_Project', 'Is_First_Client_Project', 'Default_Rate_Group', 'Perc_of_Stages_with_Fixed_Fee',\n",
    "'Project_Director',  'Project_Manager', 'Synergy_Team', 'Employment_Total_Months', 'Manager_Is_Recent', 'Perc_of_Subcontractors',\n",
    "'Project_Duration_Weeks', 'Is_Front_Loaded', 'Delivered_on_Time', 'Total_Data_Issues',\n",
    "'Client_ID', 'Client_Projects_Total_No', 'Client_Is_Repeated', 'Client_Duration_Months', 'Client_Is_Recent',\n",
    "'Stage_ID', 'Stage_Discipline', 'Recoverability', 'Profit_Measure']]\n",
    "transaction_lvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaad3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final dataframes in CSV format\n",
    "project_lvl.to_csv('csv-files/project_lvl.csv', index=False)\n",
    "stage_lvl.to_csv('csv-files/stage_lvl.csv', index=False)\n",
    "transaction_lvl.to_csv('csv-files/transaction_lvl.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of MSIN0097: Predictive Analytics.ipynb",
   "provenance": [
    {
     "file_id": "1wMnYS6Zd-TSClglZnpa6s87vKhDqcJPV",
     "timestamp": 1646991414543
    }
   ]
  },
  "interpreter": {
   "hash": "32faf87829e52f10b3379fa51fb017496aba8a2082e84bf41be67a5b199752f4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
